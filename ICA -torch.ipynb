{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4feac73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitening data...\n",
      "x_white shape: 200, 265505\n",
      "Done.\n",
      "Running INFOMAX-ICA ...\n",
      "Beginning ICA training...\n",
      "Step 1: Lrate 9.4e-04,Wchange 0.0e+00,Angle 0.00\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.linalg as tla\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "# Global constants\n",
    "EPS = 1e-16\n",
    "MAX_W = 1e8\n",
    "ANNEAL = 0.9\n",
    "MAX_STEP = 500\n",
    "MIN_LRATE = 1e-6\n",
    "W_STOP = 1e-6\n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    \"\"\"Computes the norm of a vector or the Frobenius norm of a\n",
    "    matrix_rank\n",
    "\n",
    "    \"\"\"\n",
    "    return torch.norm(x.ravel())\n",
    "\n",
    "\n",
    "class ica:\n",
    "\n",
    "    def __init__(self, n_components=10):\n",
    "        self.n_comp = n_components\n",
    "\n",
    "    def fit(self, x2d):\n",
    "        x_white, self.white, self.dewhite\\\n",
    "            = pca_whiten(x2d, self.n_comp)\n",
    "        self.mix, self.sources, self.unmix\\\n",
    "            = infomax1(x_white, self.n_comp)\n",
    "        return self.mix, self.sources\n",
    "\n",
    "\n",
    "def diagsqrts(w):\n",
    "    \"\"\"\n",
    "    Returns direct and inverse square root normalization matrices\n",
    "    \"\"\"\n",
    "    Di = torch.diag(1. / (torch.sqrt(w) + torch.finfo(float).eps))\n",
    "    D = torch.diag(torch.sqrt(w))\n",
    "    return D, Di\n",
    "\n",
    "\n",
    "def pca_whiten(x2d, n_comp, verbose=True):\n",
    "    \"\"\" data Whitening\n",
    "    *Input\n",
    "    x2d : 2d data matrix of observations by variables\n",
    "    n_comp: Number of components to retain\n",
    "    *Output\n",
    "    Xwhite : Whitened X\n",
    "    white : whitening matrix (Xwhite = np.dot(white,X))\n",
    "    dewhite : dewhitening matrix (X = np.dot(dewhite,Xwhite))\n",
    "    \"\"\"\n",
    "    x2d_demean = x2d - x2d.mean(axis=1).reshape((-1, 1))\n",
    "    NSUB, NVOX = x2d_demean.shape\n",
    "    if NSUB > NVOX:\n",
    "        cov = torch.matmul(x2d_demean.T, x2d_demean) / (NSUB - 1)\n",
    "        w, v = torch.eigh(cov, eigvals=(NVOX - n_comp, NVOX - 1))\n",
    "        D, Di = diagsqrts(w)\n",
    "        u = torch.matmul(torch.matmul(x2d_demean, v), Di)\n",
    "        x_white = v.T\n",
    "        white = torch.matmul(Di, u.T)\n",
    "        dewhite = torch.matmul(u, D)\n",
    "    else:\n",
    "        cov = torch.matmul(x2d_demean, x2d_demean.T) / (NVOX - 1)\n",
    "        w, u = tla.eigh(cov)\n",
    "        w = w[(NSUB - n_comp):]\n",
    "        u = u[:, (NSUB - n_comp):]\n",
    "        D, Di = diagsqrts(w)\n",
    "        white = torch.matmul(Di, u.T)\n",
    "        x_white = torch.matmul(white, x2d_demean)\n",
    "        dewhite = torch.matmul(u, D)\n",
    "    return (x_white, white, dewhite)\n",
    "\n",
    "\n",
    "def w_update(weights, x_white, bias1, lrate1):\n",
    "    \"\"\" Update rule for infomax\n",
    "    This function recieves parameters to update W1\n",
    "    * Input\n",
    "    W1: unmixing matrix (must be a square matrix)\n",
    "    Xwhite1: whitened data\n",
    "    bias1: current estimated bias\n",
    "    lrate1: current learning rate\n",
    "    startW1: in case update blows up it will start again from startW1\n",
    "    * Output\n",
    "    W1: updated mixing matrix\n",
    "    bias: updated bias\n",
    "    lrate1: updated learning rate\n",
    "    \"\"\"\n",
    "    device = weights.device\n",
    "    NCOMP, NVOX = x_white.shape\n",
    "    block1 = int(np.floor(np.sqrt(NVOX / 3)))\n",
    "    permute1 = torch.randperm(NVOX)\n",
    "    for start in range(0, NVOX, block1):\n",
    "        if start + block1 < NVOX:\n",
    "            tt2 = start + block1\n",
    "        else:\n",
    "            tt2 = NVOX\n",
    "            block1 = NVOX - start\n",
    "\n",
    "        unmixed = torch.matmul(\n",
    "            weights, x_white[:, permute1[start:tt2]]) + bias1\n",
    "        logit = 1 - (2 / (1 + torch.exp(-unmixed)))\n",
    "        weights = weights + lrate1 * torch.matmul(block1 * torch.eye(NCOMP, device=device) +\n",
    "                                                  torch.matmul(logit, unmixed.T), weights)\n",
    "        bias1 = bias1 + lrate1 * logit.sum(axis=1).reshape(bias1.shape)\n",
    "        # Checking if W blows up\n",
    "        if (torch.isnan(weights)).any() or torch.max(torch.abs(weights)) > MAX_W:\n",
    "            print(\"Numeric error! restarting with lower learning rate\")\n",
    "            lrate1 = lrate1 * ANNEAL\n",
    "            weights = torch.eye(NCOMP, device=device)\n",
    "            bias1 = torch.zeros((NCOMP, 1), device=device)\n",
    "            error = 1\n",
    "\n",
    "            if lrate1 > 1e-6 and \\\n",
    "               tla.matrix_rank(x_white) < NCOMP:\n",
    "                print(\"Data 1 is rank defficient\"\n",
    "                      \". I cannot compute \" +\n",
    "                      str(NCOMP) + \" components.\")\n",
    "                return (None, None, None, 1)\n",
    "\n",
    "            if lrate1 < 1e-6:\n",
    "                print(\"Weight matrix may\"\n",
    "                      \" not be invertible...\")\n",
    "                return (None, None, None, 1)\n",
    "            break\n",
    "        else:\n",
    "            error = 0\n",
    "\n",
    "    return (weights, bias1, lrate1, error)\n",
    "\n",
    "def infomax1(x_white, verbose=False):\n",
    "    \n",
    "    device = x_white.device\n",
    "    NCOMP = x_white.shape[0]\n",
    "    # Initialization\n",
    "    weights = torch.eye(NCOMP, device=device)\n",
    "    old_weights = torch.eye(NCOMP, device=device)\n",
    "    d_weigths = torch.zeros(NCOMP, device=device)\n",
    "    old_d_weights = torch.zeros(NCOMP, device=device)\n",
    "    lrate = 0.005 / np.log(NCOMP)\n",
    "    bias = torch.zeros((NCOMP, 1), device=device)\n",
    "    change = 1\n",
    "    angle_delta = 0\n",
    "    if verbose:\n",
    "        print(\"Beginning ICA training...\")\n",
    "    step = 1\n",
    "    error = 0  # Initialize error here\n",
    "\n",
    "    while step < MAX_STEP and change > W_STOP:\n",
    "\n",
    "        if error != 0:\n",
    "            step = 1\n",
    "            error = 0\n",
    "            lrate = lrate * ANNEAL if lrate is not None else 0.005 / np.log(NCOMP)\n",
    "            # ... (existing code)\n",
    "        else:\n",
    "            d_weigths = weights - old_weights\n",
    "            change = norm(d_weigths)**2\n",
    "\n",
    "            if step > 2:\n",
    "                angle_delta = torch.arccos(\n",
    "                    torch.sum(d_weigths * old_d_weights) /\n",
    "                    (norm(d_weigths) * norm(old_d_weights) + 1e-8)\n",
    "                ) * 180 / np.pi\n",
    "\n",
    "            old_weights = weights.clone()\n",
    "\n",
    "            if angle_delta > 60:\n",
    "                lrate = lrate * ANNEAL if lrate is not None else 0.005 / np.log(NCOMP)\n",
    "                # ... (existing code)\n",
    "            elif step == 1:\n",
    "                old_d_weights = d_weigths.clone()\n",
    "\n",
    "            if verbose and change < W_STOP:\n",
    "                print(\"Step %d: Lrate %.1e,\"\n",
    "                      \"Wchange %.1e,\"\n",
    "                      \"Angle %.2f\" % (step, lrate,\n",
    "                                      change, angle_delta))\n",
    "\n",
    "        step = step + 1\n",
    "\n",
    "    # A,S,W\n",
    "    return (tla.inv(weights), torch.matmul(weights, x_white), weights)\n",
    "\n",
    "\n",
    "# Single modality ICA\n",
    "\n",
    "\n",
    "def ica1(x_raw, ncomp, verbose=False):\n",
    "    '''\n",
    "    Single modality Independent Component Analysis\n",
    "    '''\n",
    "    device = x_raw.device\n",
    "    if verbose:\n",
    "        print(\"Whitening data...\")\n",
    "    x_white, _, dewhite = pca_whiten(x_raw, ncomp)\n",
    "    if verbose:\n",
    "        print('x_white shape: %d, %d' % x_white.shape)\n",
    "        print(\"Done.\")\n",
    "    if verbose:\n",
    "        print(\"Running INFOMAX-ICA ...\")\n",
    "    mixer, sources, unmixer = infomax1(x_white, verbose)\n",
    "    mixer = torch.matmul(dewhite, mixer)\n",
    "\n",
    "    scale = sources.std(axis=1).reshape((-1, 1))\n",
    "    sources = sources / scale\n",
    "    scale = scale.reshape((1, -1))\n",
    "    mixer = mixer * scale\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "    return (mixer, sources, unmixer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/data/users2/mjafarlou1/MEG/stc/sub-01/sub-01_task-RDR_run-29_meg.stc-lh.stc'\n",
    "    stc = mne.read_source_estimate(file_path)\n",
    "\n",
    "    n_components = 200\n",
    "\n",
    "    x_raw_tensor = torch.tensor(stc.data)\n",
    "    mixer, sources, unmixer = ica1(x_raw_tensor, n_components, verbose=True)\n",
    "\n",
    "   # mixer, sources, unmixer = ica1(stc.data, n_components, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029483b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 265505])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13d732c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 200])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmixer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a09014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5124, 200])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5026f908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5124, 265505)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stc.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ebb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
